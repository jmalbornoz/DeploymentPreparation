{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.- Travel Marketing Machine Learning Pipeline: Model Build \n",
    "\n",
    "There will be a notebook for each one of the Machine Learning Pipeline steps:\n",
    "\n",
    "1. Data Analysis\n",
    "2. Feature Engineering\n",
    "3. Model Build\n",
    "\n",
    "**This is the notebook for step 3: Model Build**\n",
    "\n",
    "**The purpose of these notebooks is to provide an idea of the steps that must be covered when preparing a machine learning model for deployment.**\n",
    "\n",
    "===================================================================================================\n",
    "\n",
    "## Predicting Repeat Customers in the Travel Business\n",
    "\n",
    "The aim of the project is to build a machine learning model to predict which customers of a travel agency are going to be repeat customers.\n",
    "\n",
    "### Why is this important? \n",
    "\n",
    "The travel agency is giving out too many discounted packages without ROI - they want to send discounted offers only to customers that will repeat. On the other hand, they want to reduce churn by sending targeted marketing to customers who defect.\n",
    "\n",
    "### What is the objective of the machine learning model?\n",
    "\n",
    "We aim to identify customers that will repeat using data describing each customer's socioeconomic status and interests. \n",
    "\n",
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travel marketing dataset: Initial Model Build\n",
    "\n",
    "In the following cells, we will train and tune a random forest classifier. The choice of model is arbitrary as the idea is to provide an illustration of the steps that must be followed when preparing a machine learning model for production.\n",
    "\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "We will perform model training & hyperparameter tuning in two stages:\n",
    "\n",
    "1. A coarse random search will be used to get ballpark figures for the model hyperparameters.\n",
    "2. The values used in the previous step will be used to perform a refined full grid search to find optimal hyperparameters.\n",
    "\n",
    "The criteria that will be used to tune the model is to **maximise F1 score.** This choice of metric is arbitrary.\n",
    "\n",
    "\n",
    "### Setting the seed\n",
    "\n",
    "It is important to note that we are engineering variables with the idea of deploying the model. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pretty print\n",
    "from pprint import pprint\n",
    "\n",
    "# to build the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# to assess model performance\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# to save tuned model\n",
    "import joblib\n",
    "\n",
    "# maximum number of dataframe rows and columns displayed\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# to time hyperparameter searches\n",
    "import time\n",
    "\n",
    "# random seed\n",
    "RANDOM_STATE = 801\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test set with the engineered datasets\n",
    "# we saved in the previous notebook\n",
    "\n",
    "X_train_encoded = pd.read_csv('X_train_engineered.csv')\n",
    "X_test_encoded = pd.read_csv('X_test_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 168)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 168)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we will train an tune the hyperparameters of a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Coarse random search\n",
    "\n",
    "We will first perform an initial random search to get ballpark figures for the model's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'criterion': ['gini', 'entropy'],\n",
      " 'max_depth': [10, 20, 30, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 3, 5],\n",
      " 'min_samples_split': [1, 3, 5],\n",
      " 'n_estimators': [25, 50, 75, 100, 125, 150, 175]}\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "# parameter values for a random grid search are defined #\n",
    "#########################################################\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 25, stop = 175, num = 7)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 30, num = 3)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 3, 5]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3, 5]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Metric to assess the quality of a split.\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Create parameter grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion': criterion}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a random forest classifier \n",
    "rf = RandomForestClassifier(random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random grid search to find initial hyperparameters\n",
    "\n",
    "# Random parameter search, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "# the metric that will be used for the search is F1 (please note this is an arbitrary choice)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 5, \n",
    "                               verbose=2, random_state=RANDOM_STATE, n_jobs = -1, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random grid search took 6.569512482484182  minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "time_start = time.time()\n",
    "rf_random.fit(X_train_encoded, np.ravel(y_train))\n",
    "time_end = time.time()\n",
    "print(\"Random grid search took\" ,(time_end - time_start)/60., ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 3,\n",
      " 'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "# These are the best hyperparameters found by the random search - these values will guide the \n",
    "# full search performed in the next section\n",
    "pprint(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.- Random Search Model Evaluation\n",
    "\n",
    "We will now compare a base model (the one with default parameters) with the model that resulted from the random search; the idea is to verify that there has been an improvement in performance after the initial hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    \n",
    "    \"\"\"    \n",
    "    A function to assess a classification model's performance\n",
    "    \n",
    "    -------------------------------\n",
    "    model: model to be evaluated\n",
    "    test_features: features to be fed to the model\n",
    "    test_labels: known outcomes\n",
    "    \"\"\"\n",
    "    \n",
    "    # precision & recall\n",
    "    predictions = model.predict(test_features)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)   \n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "   \n",
    "    # log-loss\n",
    "    probabilities = model.predict_proba(test_features)\n",
    "    \n",
    "    # keep the predictions for class 1 only\n",
    "    probabilities = probabilities[:, 1]\n",
    "    \n",
    "    # calculate log loss\n",
    "    loss = log_loss(test_labels, probabilities)\n",
    "    \n",
    "    print('Precision = {:0.2f}.'.format(precision))\n",
    "    print('Recall = {:0.2f}.'.format(recall))\n",
    "    print('F1 = {:0.4f}.'.format(f1))\n",
    "    print('LogLoss = {:0.4f}.'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=801)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "base_model.fit(X_train_encoded, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the base model on the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.00.\n",
      "Recall = 1.00.\n",
      "F1 = 0.9992.\n",
      "LogLoss = 0.0833.\n"
     ]
    }
   ],
   "source": [
    "evaluate(base_model, X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.98.\n",
      "Recall = 0.17.\n",
      "F1 = 0.2893.\n",
      "LogLoss = 0.3570.\n"
     ]
    }
   ],
   "source": [
    "evaluate(base_model, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the model resulting from our initial random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.00.\n",
      "Recall = 1.00.\n",
      "F1 = 0.9992.\n",
      "LogLoss = 0.0410.\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_random, X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.00.\n",
      "Recall = 0.17.\n",
      "F1 = 0.2956.\n",
      "LogLoss = 0.3523.\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_random, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced by the increase of F1 values in the test set, there is an improvement in performance after the random grid search. Therefore, the corresponding hyperparameters are a good starting point for a full grid search on a finer grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Full grid search\n",
    "\n",
    "Now that we know approximate values for the hyperparameters we can run a full grid search using these as guidance for search ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [False],\n",
      " 'criterion': ['gini'],\n",
      " 'max_depth': [10, 20, 30, None],\n",
      " 'max_features': ['auto'],\n",
      " 'min_samples_leaf': [1, 2, 3, 4],\n",
      " 'min_samples_split': [1, 2, 3, 4],\n",
      " 'n_estimators': [60, 65, 70, 75, 80, 85]}\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# parameter values for a full grid search are defined #\n",
    "#######################################################\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [60, 65, 70, 75, 80, 85]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 3, 4]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False]\n",
    "\n",
    "# Metric to assess the quality of a split.\n",
    "criterion = ['gini']\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion': criterion}\n",
    "\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search \n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 381 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 664 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1029 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1474 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full grid search took 10.993253016471863  minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit the full grid search model\n",
    "time_start = time.time()\n",
    "grid_search.fit(X_train_encoded, np.ravel(y_train))\n",
    "time_end = time.time()\n",
    "print(\"Full grid search took\" ,(time_end - time_start)/60., ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 65}\n"
     ]
    }
   ],
   "source": [
    "# These are the optimal hyperparameters found by the full grid search\n",
    "pprint(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.- Full Grid Search Model Evaluation\n",
    "\n",
    "We now want to evaluate the model resulting from the full grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.00.\n",
      "Recall = 1.00.\n",
      "F1 = 0.9992.\n",
      "LogLoss = 0.0004.\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_grid, X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.98.\n",
      "Recall = 0.19.\n",
      "F1 = 0.3158.\n",
      "LogLoss = 0.4092.\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_grid, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an improvement in the F1 score after the full grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Saving the tuned model\n",
    "\n",
    "We will save the best model resulting from the full grid search as a pickle file that will be invoked by the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_grid, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conclude the model building section for this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "583px",
    "left": "0px",
    "right": "1324px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
